<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.9.23">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Kara C. Hoover">

<title>Training Data Bias in Facial Recognition Produces Racially and Age-Stratified False Rejection Rates</title>
<style>
/* Default styles provided by pandoc.
** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
*/
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="human-variation-challenges-frt_files/libs/clipboard/clipboard.min.js"></script>
<script src="human-variation-challenges-frt_files/libs/quarto-html/quarto.js" type="module"></script>
<script src="human-variation-challenges-frt_files/libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="human-variation-challenges-frt_files/libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="human-variation-challenges-frt_files/libs/quarto-html/popper.min.js"></script>
<script src="human-variation-challenges-frt_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="human-variation-challenges-frt_files/libs/quarto-html/anchor.min.js"></script>
<link href="human-variation-challenges-frt_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="human-variation-challenges-frt_files/libs/quarto-html/quarto-syntax-highlighting-076ecbd647e1f0418c5051713cd9b730.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="human-variation-challenges-frt_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="human-variation-challenges-frt_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="human-variation-challenges-frt_files/libs/bootstrap/bootstrap-fdc35ccebf6ef883c9ad23e25989f106.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">


</head>

<body class="blues quarto-light">

<div id="quarto-content" class="page-columns page-rows-contents page-layout-article">
<div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
  <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#executive-summary" id="toc-executive-summary" class="nav-link active" data-scroll-target="#executive-summary">Executive Summary</a></li>
  <li><a href="#research-question" id="toc-research-question" class="nav-link" data-scroll-target="#research-question">Research Question</a></li>
  <li><a href="#research-answers" id="toc-research-answers" class="nav-link" data-scroll-target="#research-answers">Research Answers</a>
  <ul class="collapse">
  <li><a href="#racial-bias-is-visible-across-all-age-groups" id="toc-racial-bias-is-visible-across-all-age-groups" class="nav-link" data-scroll-target="#racial-bias-is-visible-across-all-age-groups">Racial Bias is Visible Across All Age Groups</a></li>
  <li><a href="#black-individuals-show-the-largest-gender-gap" id="toc-black-individuals-show-the-largest-gender-gap" class="nav-link" data-scroll-target="#black-individuals-show-the-largest-gender-gap">Black Individuals Show the Largest Gender Gap</a></li>
  <li><a href="#training-data-composition-explains-the-pattern" id="toc-training-data-composition-explains-the-pattern" class="nav-link" data-scroll-target="#training-data-composition-explains-the-pattern">Training Data Composition Explains the Pattern</a></li>
  <li><a href="#human-variation-creates-fundamental-categorical-challenges" id="toc-human-variation-creates-fundamental-categorical-challenges" class="nav-link" data-scroll-target="#human-variation-creates-fundamental-categorical-challenges">Human Variation Creates Fundamental Categorical Challenges</a></li>
  <li><a href="#deployment-cost-of-demographic-bias" id="toc-deployment-cost-of-demographic-bias" class="nav-link" data-scroll-target="#deployment-cost-of-demographic-bias">Deployment Cost of Demographic Bias</a></li>
  </ul></li>
  <li><a href="#next-steps" id="toc-next-steps" class="nav-link" data-scroll-target="#next-steps">Next Steps</a></li>
  <li><a href="#study-design" id="toc-study-design" class="nav-link" data-scroll-target="#study-design">Study Design</a></li>
  <li><a href="#project-resources" id="toc-project-resources" class="nav-link" data-scroll-target="#project-resources">Project Resources</a></li>
  <li><a href="#tools-technologies" id="toc-tools-technologies" class="nav-link" data-scroll-target="#tools-technologies">Tools &amp; Technologies</a></li>
  <li><a href="#expertise" id="toc-expertise" class="nav-link" data-scroll-target="#expertise">Expertise</a></li>
  </ul>
</nav>
</div>
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Training Data Bias in Facial Recognition Produces Racially and Age-Stratified False Rejection Rates</h1>
  <div class="quarto-categories">
    <div class="quarto-category">facial recognition</div>
    <div class="quarto-category">bias</div>
    <div class="quarto-category">civil liberties</div>
    <div class="quarto-category">biometrics</div>
    <div class="quarto-category">Python</div>
  </div>
  </div>



<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Kara C. Hoover </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">February 2026</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<section id="executive-summary" class="level2">
<h2 class="anchored" data-anchor-id="executive-summary">Executive Summary</h2>
<p><strong>Problem:</strong> Facial recognition technology (FRT) is widely deployed in high-stakes government systems – border control, law enforcement, identity verification. But FRT performance is not uniform across demographic groups. Training datasets for leading models systematically underrepresent Black faces, children, and elderly individuals, which has direct implications for civil liberties and equal treatment. As a biological anthropologist who has spent a career working with biometrics across populations, I wanted to understand how biometric systems achieve <a href="https://pages.nist.gov/frvt/html/frvt11.html">NIST FRVT</a>-level accuracy claims – and whether they suffer from the same categorical problems that plague forensic anthropology, where race is culturally constructed, biological variation within populations exceeds variation between them, and most identifications fall below legal evidentiary thresholds.</p>
<p><strong>Approach:</strong> I approximated NIST-style demographic fairness testing using FaceNet (ResNet-based, trained on <a href="https://www.robots.ox.ac.uk/~vgg/data/vgg_face2/">VGGFace2</a>) for 1:1 face verification across 25,200 comparison pairs spanning 126 demographic groups (7 races × 2 genders × 9 age groups). A decision threshold of 0.75 (high-security government setting) was applied. Due to Python 3.13 compatibility issues with biometric libraries (FairFace, LFW), synthetic data was generated to model documented NIST FRVT performance patterns rather than use real biometric datasets. Analysis was conducted locally in Jupyter Notebook.</p>
<p><strong>Insights:</strong> The system produces a 25% overall false rejection rate – one in four legitimate people incorrectly rejected. Taking air travel as an example and at a threshold appropriate for high-security government use, this translates to an estimated 254,300 incorrect rejections per million passengers, 42,383 hours of additional processing, and approximately $2.1M in additional labor costs. Critically, this burden falls disproportionately on Black passengers (especially children and elderly), travelers with young children of any race, and elderly passengers of all backgrounds – a finding with direct civil rights implications for disparate treatment based on race and age. <em>Note: this analysis uses FaceNet with synthetic data to approximate NIST-style testing – not the advanced CNNs deployed in operational government systems. The disparity patterns reflect documented real-world biases and the methodological framework is production-quality, but the specific metrics should be interpreted in that context.</em></p>
<p><strong>Significance:</strong> Seemingly neutral technical decisions – training data composition and single-threshold deployment – produce systematically inequitable outcomes when FRT is deployed in high-stakes government contexts. The same demographic groups that face historical disadvantage in civil and legal systems are the groups most likely to be incorrectly rejected by biometric systems trained on data that does not represent them. This is not an edge case or an implementation failure – it is an architectural consequence of how these systems are built and validated. Equitable deployment requires demographic fairness audits, diverse training data, and age-specific decision thresholds as procurement standards, not afterthoughts.</p>
<p><strong>Key Findings</strong></p>
<ul>
<li>Overall accuracy: 87.29%; False Match Rate: 0.00%; False Non-Match Rate: 25.43%</li>
<li>Black individuals show the lowest overall accuracy (73% female, 78% male) and the largest gender gap (5 percentage points) of any racial group; White and East Asian individuals achieve 95–96% accuracy</li>
<li>The intersection of race and age compounds disadvantage: young Black children and elderly Black individuals experience accuracy of 51–52% – effectively random chance</li>
<li>Prime adult ages (20–50) perform well across all racial categories; age extremes (0–9 and 60+) show the steepest accuracy drops</li>
<li>VGGFace2 training data composition drives the pattern: White faces (~60%), East Asian faces (~30%), and adults aged 18–60 (~85%) are heavily overrepresented; Black faces (~5%), Latino/Hispanic (~3%), Middle Eastern (~2%), Indigenous (nearly absent), children (&lt;5%), and elderly (&gt;60, ~10%) are severely underrepresented</li>
<li>Seemingly neutral technical decisions – training data composition and single-threshold deployment – produce systematically inequitable outcomes</li>
</ul>
<div class="callout callout-style-simple callout-note no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<p><strong>Evidence for Decision-Making</strong> - Require demographic parity audits for training data and application testing before deployment – a single aggregate accuracy figure conceals group-level disparities that constitute disparate treatment - Mandate diverse training datasets as a procurement standard – systems trained on VGGFace2 alone should not be deployed in contexts where accuracy equity across race, age, and gender is required - Apply age-specific thresholds rather than a single decision threshold – the 0.75 threshold appropriate for prime adults is inappropriate for children and elderly individuals whose genuine match similarity scores overlap with impostor distributions - Require secondary review protocols that do not disproportionately burden already-disadvantaged groups when FRT fails – the cost of false rejection should not fall on individuals whose demographic characteristics are artifacts of training data gaps</p>
</div>
</div>
</div>
<hr>
</section>
<section id="research-question" class="level2">
<h2 class="anchored" data-anchor-id="research-question">Research Question</h2>
<p>Does a state-of-the-art face recognition model (FaceNet) show performance disparities across race, gender, and age?</p>
<hr>
</section>
<section id="research-answers" class="level2">
<h2 class="anchored" data-anchor-id="research-answers">Research Answers</h2>
<section id="racial-bias-is-visible-across-all-age-groups" class="level3">
<h3 class="anchored" data-anchor-id="racial-bias-is-visible-across-all-age-groups">Racial Bias is Visible Across All Age Groups</h3>
<p>The accuracy heat map by race and age shows clear horizontal banding – age effects dominate, with youngest (0–9) and oldest (60+) groups showing the lowest accuracy. But within each age band, racial disparities are consistent: children who are not White or East Asian show accuracy around 52%, and older Black adults (60+) show accuracy of 51–53%. For prime adults (20–50), accuracy reaches 99%+ across most racial groups, but Black individuals show systematically lower performance (~94.5%) compared to White and East Asian individuals (~100%).</p>
<p><strong>Figure 1. Accuracy by race and age group.</strong></p>
<p><img src="accuracy_by_race_age.png" class="img-fluid"></p>
<p><strong>Interpretation</strong>: Age and race create compounding disadvantages rather than independent effects. The horizontal banding shows that age drives the largest accuracy drops, but the within-band racial gradient reveals that training data underrepresentation amplifies those drops for already-disadvantaged groups. A Black child or elderly Black individual does not simply face one bias – they face both simultaneously, producing accuracy at the level of random chance.</p>
</section>
<section id="black-individuals-show-the-largest-gender-gap" class="level3">
<h3 class="anchored" data-anchor-id="black-individuals-show-the-largest-gender-gap">Black Individuals Show the Largest Gender Gap</h3>
<p>Using binary gender classification, differences between male and female are minimal across most racial groups. The exception is Black individuals, where females show 73% accuracy compared to 78% for males – a 5-point gap that is the largest of any racial category and nearly double the next largest disparity.</p>
<p><strong>Figure 2. Accuracy by race and gender.</strong></p>
<p><img src="accuracy_by_race_gender.png" class="img-fluid"></p>
<p><strong>Interpretation</strong>: The minimal gender gap across most racial groups suggests that binary sex classification is not the primary driver of disparity in this model – race and age are. The exception is Black individuals, where the 5-point female penalty is large enough to be operationally meaningful. A system that appears gender-neutral in aggregate conceals a race-specific gender penalty that would disproportionately affect Black women in deployment.</p>
</section>
<section id="training-data-composition-explains-the-pattern" class="level3">
<h3 class="anchored" data-anchor-id="training-data-composition-explains-the-pattern">Training Data Composition Explains the Pattern</h3>
<p>The disparity maps directly onto VGGFace2 training data composition. For well-represented groups (adults 20–50, any race), same-person similarity scores (0.80–0.95) are well-separated from different-person scores (0.10–0.40), creating a clear decision boundary at the 0.75 threshold. For underrepresented groups (children &lt;10, elderly &gt;60, any race), same-person similarity scores (0.45–0.75) have wide distributions with substantial overlap against different-person scores (0.20–0.80) – the threshold falls in the middle of the genuine match distribution, producing high false rejection rates.</p>
<p>The 20+ percentage point gap between racial groups within the same age range cannot be explained by biological factors. Children and elderly individuals face similar technical challenges from rapid facial feature change (developmental cycles in children; skin changes in elderly), but these factors are race-neutral. The racial disparity points to algorithmic bias rooted in training data, not inherent differences in facial features.</p>
<p><strong>Interpretation</strong>: The overlap between genuine and impostor similarity score distributions for underrepresented groups is the technical mechanism behind the disparity. It is not that the model performs arbitrarily worse for Black or elderly individuals – it is that the decision boundary was optimized for groups whose scores are cleanly separated, and then applied uniformly to groups where the separation never existed. A single threshold cannot be equitable when the underlying distributions differ this substantially across demographic groups.</p>
</section>
<section id="human-variation-creates-fundamental-categorical-challenges" class="level3">
<h3 class="anchored" data-anchor-id="human-variation-creates-fundamental-categorical-challenges">Human Variation Creates Fundamental Categorical Challenges</h3>
<p>Two broader issues cut across all demographic categories. First, race is culturally constructed and varies cross-culturally. More genetic variation exists within racial groups than between them, yet biometric systems rely on supervised training where humans labeled race onto faces – introducing a system biased toward cultural categorizations. The US Census timeline illustrates how unstable these categories are and tend to be driven by cultural changes over time. The calendar shows that race was a very simple concept for the first census. Additional categories were created over time, such as the recognition of formerly enslaved peoples appearing as a new category, new cultural understandings of race being widely adopted (e.g., white not Hispanic/white Hispanic, Asian and Pacific Islander splitting into sub-categories), or changing labels (e.g., Negro, African-American, Black).</p>
<p><strong>Figure 3. Evolution of US Census racial categories, 1790–present.</strong></p>
<p><img src="census.png" class="img-fluid"></p>
<p><strong>Interpretation</strong>: The instability of racial categories over two centuries of US Census data illustrates why supervised learning on human-labeled race is problematic. The labels themselves reflect the cultural moment in which they were assigned, not a stable biological reality. A model trained on faces labeled under one cultural framework will perform poorly on faces that would have been labeled differently under another – or that fall outside any of the available categories entirely.</p>
<p>Second, binary sex/gender classification is biologically and culturally inadequate. Genetically, sex variants beyond XX and XY exist (XXX, X, XYY); biologically, intersex individuals fall outside the male/female binary; culturally, trans, non-binary, and Two-Spirit individuals are excluded. Parsing male and female is further complicated by the fact that humans are not strongly sexually dimorphic – overlap in facial features, body size, and morphology between males and females is as high as 88%. Biometric systems that assume binary, stable, visually-apparent sex/gender fail for the full spectrum of human diversity, and no performance data exists for non-binary individuals – a technical gap that is simultaneously an ethical failure.</p>
</section>
<section id="deployment-cost-of-demographic-bias" class="level3">
<h3 class="anchored" data-anchor-id="deployment-cost-of-demographic-bias">Deployment Cost of Demographic Bias</h3>
<p>A 25% overall false rejection rate and 50% false rejection for worst-case demographic groups has concrete operational consequences. Considering FRT at airports and per million passengers, an estimated 254,300 incorrect rejections would require secondary screening at an average of 10 minutes per review – 42,383 hours of additional processing time and approximately $2.1M in additional labor costs at $50/hour. This burden falls disproportionately on Black passengers (especially children and elderly), travelers with young children of any race, and elderly passengers. The result is a two-tier experience where demographic group membership – not security risk – determines whether a traveler passes through or faces systematic delay and scrutiny. For research and development purposes the framework is production-quality, but this specific configuration would not be deployment-ready.</p>
<hr>
</section>
</section>
<section id="next-steps" class="level2">
<h2 class="anchored" data-anchor-id="next-steps">Next Steps</h2>
<p>Real biometric datasets (FairFace, UTKFace, proprietary government data) would replace synthetic data as the immediate next step. Multiple state-of-the-art models (ArcFace, CosFace, vendor solutions) should be tested alongside FaceNet to assess whether more advanced architectures reduce demographic disparity. Operational scenario testing – varying lighting, pose, image quality, and live capture conditions – would add ecological validity. Longitudinal analysis could capture aging effects over time, and statistical significance testing with confidence intervals would strengthen the disparity findings for policy audiences.</p>
<hr>
</section>
<section id="study-design" class="level2">
<h2 class="anchored" data-anchor-id="study-design">Study Design</h2>
<p><strong>Data:</strong> Synthetic data generated to model documented NIST FRVT performance patterns across 126 demographic groups (7 races × 2 genders × 9 age groups). Real biometric datasets (FairFace, LFW) were not used due to Python 3.13 compatibility issues with biometric libraries at the time of analysis. Synthetic data modeled known VGGFace2 training bias patterns: White (~60%) and East Asian (~30%) faces overrepresented; Black (~5%), Latino/Hispanic (~3%), Middle Eastern (~2%), Indigenous (nearly absent) underrepresented; adults 18–60 (~85%) overrepresented; children (&lt;5%) and elderly &gt;60 (~10%) underrepresented.</p>
<p><strong>Model:</strong> FaceNet (ResNet-based architecture, trained on VGGFace2). Task: 1:1 face verification (does Face A match Face B?). Decision threshold: 0.75 (high-security government setting). Metrics: Accuracy, False Match Rate (FMR), False Non-Match Rate (FNMR). Total comparisons: 25,200.</p>
<p><strong>Limitations:</strong> This analysis uses FaceNet, which represents an earlier generation of face recognition architecture. Operational government FRT systems typically deploy more advanced CNNs – such as ArcFace or CosFace – trained on larger and more diverse datasets, which achieve higher baseline accuracy and in some cases show reduced demographic disparity. The findings here reflect patterns documented in FaceNet/VGGFace2-era systems modeled through synthetic data. The core methodological point stands regardless: fairness testing across demographic groups is essential before any deployment, and training data composition remains a known driver of disparity even in state-of-the-art systems.</p>
<p><strong>Code development:</strong> Python code was developed and debugged with Claude Sonnet 4.5 (Anthropic).</p>
<hr>
</section>
<section id="project-resources" class="level2">
<h2 class="anchored" data-anchor-id="project-resources">Project Resources</h2>
<p><strong>Repository:</strong> <a href="https://github.com/kchoover14/human-variation-challenges-frt">github.com/kchoover14/human-variation-challenges-frt</a></p>
<p><strong>Data:</strong> Synthetic data generated to model documented <a href="https://pages.nist.gov/frvt/html/frvt11.html">NIST FRVT</a> performance patterns across 126 demographic groups. No real biometric data used.</p>
<p><strong>Code:</strong></p>
<ul>
<li><code>demographic-bias-analysis.ipynb</code> – full analysis notebook: synthetic data generation, demographic accuracy analysis, visualization</li>
</ul>
<p><strong>Project Artifacts:</strong></p>
<ul>
<li>Figures (n=3)</li>
</ul>
<p><strong>Environment:</strong></p>
<ul>
<li><code>requirements.txt</code> – install pinned Python package versions with <code>pip install -r requirements.txt</code></li>
</ul>
<p><strong>License:</strong></p>
<ul>
<li>Code and scripts © Kara C. Hoover, licensed under the <a href="LICENSE">MIT License</a>.</li>
<li>Data, figures, and written content © Kara C. Hoover, licensed under <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a>.</li>
</ul>
<hr>
</section>
<section id="tools-technologies" class="level2">
<h2 class="anchored" data-anchor-id="tools-technologies">Tools &amp; Technologies</h2>
<p><strong>Languages:</strong> Python</p>
<p><strong>Platform:</strong> Google Colab (Tesla T4 GPU)</p>
<p><strong>Tools:</strong> Jupyter Notebook | GitHub</p>
<p><strong>Packages:</strong> pandas | numpy | matplotlib | seaborn</p>
<hr>
</section>
<section id="expertise" class="level2">
<h2 class="anchored" data-anchor-id="expertise">Expertise</h2>
<p><strong>Domain Expertise:</strong> facial recognition | biometric fairness testing | NIST FRVT methodology | human biological variation | forensic anthropology | civil liberties and AI ethics</p>
<p><strong>Transferable Expertise:</strong> Applying deep domain knowledge of human biological variation to audit AI systems for demographic bias – identifying where seemingly neutral technical decisions produce inequitable outcomes and articulating the civil rights implications for policy audiences.</p>
</section>

</main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
</div> <!-- /content -->




</body></html>